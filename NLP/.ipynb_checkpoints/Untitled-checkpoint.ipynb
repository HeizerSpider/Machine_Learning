{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pre_processing(column):\n",
    "#     \"\"\"\n",
    "#     removing html tags, punctuations and numbers, multiple spaces\n",
    "#     \"\"\"\n",
    "    \n",
    "#     txt = \"\".join([c for c in column if c not in string.punctuation])\n",
    "#     return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production br The filming t...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically theres a family where a little boy J...  negative\n",
      "4  Petter Matteis Love in the Time of Money is a ...  positive\n",
      "5  Probably my alltime favorite movie a story of ...  positive\n",
      "6  I sure would like to see a resurrection of a u...  positive\n",
      "7  This show was an amazing fresh innovative idea...  negative\n",
      "8  Encouraged by the positive comments about this...  negative\n",
      "9  If you like original gut wrenching laughter yo...  positive\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file = pd.read_csv(\"/Users/heizer/Desktop/CDS_Lab/Week_6/IMDB_Dataset.csv\")\n",
    "    print(file.head())\n",
    "    file['review'] = file['review'].apply(lambda x: pre_processing(x))\n",
    "    print(file.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(column):\n",
    "    \"\"\"\n",
    "    removing html tags, punctuations and numbers, multiple spaces\n",
    "    \"\"\"\n",
    "    list = [character for character in column]\n",
    "    #removing html tags\n",
    "    try:\n",
    "        for i in range(len(list)):\n",
    "            if list[i] == '<':\n",
    "                #remove this character all characters after till '>' is reached\n",
    "                while list[i] != \">\":\n",
    "                    list.pop(i)\n",
    "                list[i] = \" \"\n",
    "    except:\n",
    "        #end of string reached\n",
    "        pass\n",
    "    list = [p for p in list if p not in string.punctuation]\n",
    "    list = [d for d in list if d not in string.digits]\n",
    "    try:\n",
    "        for i in range(len(list)):\n",
    "            if list[i] == ' ' and list[i+1] == \" \":\n",
    "                #remove this character all characters after till '>' is reached\n",
    "                while list[i+1] == \" \":\n",
    "                    list.pop(i)\n",
    "    except:\n",
    "        # End of string reached\n",
    "        pass\n",
    "    return \"\".join(list)\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(tokens):\n",
    "    wordfreq = {}\n",
    "    for i in tokens:\n",
    "        if i[0] not in wordfreq.keys():\n",
    "            wordfreq[i[0]] = 1\n",
    "        else:\n",
    "            wordfreq[i[0]] += 1\n",
    "    # most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n",
    "    return wordfreq\n",
    "\n",
    "def feature_selector(features):\n",
    "    #features is a dictionary\n",
    "    list = []\n",
    "    for i in range(0,100):\n",
    "        list.append(features[i])\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello my name is thats right hello \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "yes = \"hello my name is <hello> thats right !@#%! 131 hello 349012\"\n",
    "print(pre_processing(yes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'test', 'file']\n",
      "[[1], [4], [2], [3], [5]]\n",
      "['this', 'isnt', 'a', 'test']\n",
      "[[1], [6], [2], [3]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ', char_level=False, oov_token=None, document_count=0)\n",
    "\n",
    "test = \"this is a test file\"\n",
    "test2 = \"this isnt a test\"\n",
    "test = text_to_word_sequence(test)\n",
    "test2 = text_to_word_sequence(test2)\n",
    "\n",
    "tokenizer.fit_on_texts(test)\n",
    "tokenizer.fit_on_texts(test2)\n",
    "testing = tokenizer.texts_to_sequences(test)\n",
    "testing2 = tokenizer.texts_to_sequences(test2)\n",
    "# testing = tokenizer.texts_to_sequences(test)\n",
    "print(test)\n",
    "print(testing)\n",
    "print(test2)\n",
    "print(testing2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  One of the other reviewers has mentioned that ...          1\n",
      "1  A wonderful little production br The filming t...          1\n",
      "2  I thought this was a wonderful way to spend ti...          1\n",
      "3  Basically theres a family where a little boy J...          0\n",
      "4  Petter Matteis Love in the Time of Money is a ...          1\n",
      "5  Probably my alltime favorite movie a story of ...          1\n",
      "6  I sure would like to see a resurrection of a u...          1\n",
      "7  This show was an amazing fresh innovative idea...          0\n",
      "8  Encouraged by the positive comments about this...          0\n",
      "9  If you like original gut wrenching laughter yo...          1\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import heapq\n",
    "\n",
    "import string\n",
    "\n",
    "file = pd.read_csv(\"/Users/heizer/Desktop/CDS_Lab/Week_6/IMDB_Dataset.csv\")\n",
    "file['review'] = file['review'].apply(lambda x: pre_processing(x))\n",
    "file['sentiment'] = file['sentiment'].apply(lambda x: 1 if x=='positive' else 0)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ', char_level=False, oov_token=None, document_count=0)\n",
    "\n",
    "print(file.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...          1   \n",
      "1  A wonderful little production br The filming t...          1   \n",
      "2  I thought this was a wonderful way to spend ti...          1   \n",
      "3  Basically theres a family where a little boy J...          0   \n",
      "4  Petter Matteis Love in the Time of Money is a ...          1   \n",
      "5  Probably my alltime favorite movie a story of ...          1   \n",
      "6  I sure would like to see a resurrection of a u...          1   \n",
      "7  This show was an amazing fresh innovative idea...          0   \n",
      "8  Encouraged by the positive comments about this...          0   \n",
      "9  If you like original gut wrenching laughter yo...          1   \n",
      "\n",
      "                                           sequences  \n",
      "0  [one, of, the, other, reviewers, has, mentione...  \n",
      "1  [a, wonderful, little, production, br, the, fi...  \n",
      "2  [i, thought, this, was, a, wonderful, way, to,...  \n",
      "3  [basically, theres, a, family, where, a, littl...  \n",
      "4  [petter, matteis, love, in, the, time, of, mon...  \n",
      "5  [probably, my, alltime, favorite, movie, a, st...  \n",
      "6  [i, sure, would, like, to, see, a, resurrectio...  \n",
      "7  [this, show, was, an, amazing, fresh, innovati...  \n",
      "8  [encouraged, by, the, positive, comments, abou...  \n",
      "9  [if, you, like, original, gut, wrenching, laug...  \n"
     ]
    }
   ],
   "source": [
    "file['sequences'] = file['review'].apply(lambda x: text_to_word_sequence(x))\n",
    "print(file.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...          1   \n",
      "1  A wonderful little production br The filming t...          1   \n",
      "2  I thought this was a wonderful way to spend ti...          1   \n",
      "3  Basically theres a family where a little boy J...          0   \n",
      "4  Petter Matteis Love in the Time of Money is a ...          1   \n",
      "5  Probably my alltime favorite movie a story of ...          1   \n",
      "6  I sure would like to see a resurrection of a u...          1   \n",
      "7  This show was an amazing fresh innovative idea...          0   \n",
      "8  Encouraged by the positive comments about this...          0   \n",
      "9  If you like original gut wrenching laughter yo...          1   \n",
      "\n",
      "                                           sequences  \\\n",
      "0  [one, of, the, other, reviewers, has, mentione...   \n",
      "1  [a, wonderful, little, production, br, the, fi...   \n",
      "2  [i, thought, this, was, a, wonderful, way, to,...   \n",
      "3  [basically, theres, a, family, where, a, littl...   \n",
      "4  [petter, matteis, love, in, the, time, of, mon...   \n",
      "5  [probably, my, alltime, favorite, movie, a, st...   \n",
      "6  [i, sure, would, like, to, see, a, resurrectio...   \n",
      "7  [this, show, was, an, amazing, fresh, innovati...   \n",
      "8  [encouraged, by, the, positive, comments, abou...   \n",
      "9  [if, you, like, original, gut, wrenching, laug...   \n",
      "\n",
      "                                           tokenized  \n",
      "0  [[27], [4], [1], [76], [], [43], [], [11], [99...  \n",
      "1  [[2], [], [], [], [], [1], [], [], [6], [51], ...  \n",
      "2  [[9], [], [10], [12], [2], [], [96], [5], [], ...  \n",
      "3  [[], [], [2], [], [], [2], [], [], [], [], [],...  \n",
      "4  [[], [], [], [7], [1], [58], [4], [], [6], [2]...  \n",
      "5  [[], [52], [], [], [16], [2], [65], [4], [], [...  \n",
      "6  [[9], [], [56], [37], [5], [63], [2], [], [4],...  \n",
      "7  [[10], [], [12], [32], [], [], [], [], [7], [1...  \n",
      "8  [[], [31], [1], [], [], [41], [10], [18], [19]...  \n",
      "9  [[42], [21], [37], [], [], [], [], [21], [75],...  \n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(file.sequences)\n",
    "file['tokenized'] = file['sequences'].apply(lambda x: tokenizer.texts_to_sequences(x))\n",
    "print(file.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fcc1866b071b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_dict'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/opencv/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3847\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3848\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3849\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-fcc1866b071b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_dict'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0bb50a1d9e81>\u001b[0m in \u001b[0;36mcounter\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mwordfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mwordfreq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "file['token_dict'] = file['tokenized'].apply(lambda x: counter(x))\n",
    "print(file.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['features'] = file['token_dict'].apply(lambda x: feature_selector(x))\n",
    "print(file.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
