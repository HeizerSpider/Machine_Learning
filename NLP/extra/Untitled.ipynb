{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(column):\n",
    "    \"\"\"\n",
    "    removing html tags, punctuations and numbers, multiple spaces\n",
    "    \"\"\"\n",
    "    list = [character for character in column]\n",
    "    #removing html tags\n",
    "    try:\n",
    "        for i in range(len(list)):\n",
    "            if list[i] == '<':\n",
    "                #remove this character all characters after till '>' is reached\n",
    "                while list[i] != \">\":\n",
    "                    list.pop(i)\n",
    "                list[i] = \" \"\n",
    "    except:\n",
    "        #end of string reached\n",
    "        pass\n",
    "    list = [p for p in list if p not in string.punctuation]\n",
    "    list = [d for d in list if d not in string.digits]\n",
    "    try:\n",
    "        for i in range(len(list)):\n",
    "            if list[i] == ' ' and list[i+1] == \" \":\n",
    "                #remove this character all characters after till '>' is reached\n",
    "                while list[i+1] == \" \":\n",
    "                    list.pop(i)\n",
    "    except:\n",
    "        # End of string reached\n",
    "        pass\n",
    "    return \"\".join(list)\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(tokens):\n",
    "    wordfreq = {}\n",
    "    for i in tokens:\n",
    "        if i[0] not in wordfreq.keys():\n",
    "            wordfreq[i[0]] = 1\n",
    "        else:\n",
    "            wordfreq[i[0]] += 1\n",
    "    # most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n",
    "    return wordfreq\n",
    "\n",
    "def feature_selector(features):\n",
    "    #features is a dictionary\n",
    "    list = []\n",
    "    for i in range(0,100):\n",
    "        list.append(features[i])\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello my name is thats right hello \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "yes = \"hello my name is <hello> thats right !@#%! 131 hello 349012\"\n",
    "print(pre_processing(yes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'test', 'file']\n",
      "[[1], [4], [2], [3], [5]]\n",
      "['this', 'isnt', 'a', 'test']\n",
      "[[1], [6], [2], [3]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ', char_level=False, oov_token=None, document_count=0)\n",
    "\n",
    "test = \"this is a test file\"\n",
    "test2 = \"this isnt a test\"\n",
    "test = text_to_word_sequence(test)\n",
    "test2 = text_to_word_sequence(test2)\n",
    "\n",
    "tokenizer.fit_on_texts(test)\n",
    "tokenizer.fit_on_texts(test2)\n",
    "testing = tokenizer.texts_to_sequences(test)\n",
    "testing2 = tokenizer.texts_to_sequences(test2)\n",
    "# testing = tokenizer.texts_to_sequences(test)\n",
    "print(test)\n",
    "print(testing)\n",
    "print(test2)\n",
    "print(testing2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  One of the other reviewers has mentioned that ...          1\n",
      "1  A wonderful little production br The filming t...          1\n",
      "2  I thought this was a wonderful way to spend ti...          1\n",
      "3  Basically theres a family where a little boy J...          0\n",
      "4  Petter Matteis Love in the Time of Money is a ...          1\n",
      "5  Probably my alltime favorite movie a story of ...          1\n",
      "6  I sure would like to see a resurrection of a u...          1\n",
      "7  This show was an amazing fresh innovative idea...          0\n",
      "8  Encouraged by the positive comments about this...          0\n",
      "9  If you like original gut wrenching laughter yo...          1\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import heapq\n",
    "\n",
    "import string\n",
    "\n",
    "file = pd.read_csv(\"/Users/heizer/Desktop/CDS_Lab/Week_6/IMDB_Dataset.csv\")\n",
    "file['review'] = file['review'].apply(lambda x: pre_processing(x))\n",
    "file['sentiment'] = file['sentiment'].apply(lambda x: 1 if x=='positive' else 0)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=80000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ', char_level=False, oov_token=None, document_count=0)\n",
    "\n",
    "print(file.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...          1   \n",
      "1  A wonderful little production br The filming t...          1   \n",
      "2  I thought this was a wonderful way to spend ti...          1   \n",
      "3  Basically theres a family where a little boy J...          0   \n",
      "4  Petter Matteis Love in the Time of Money is a ...          1   \n",
      "5  Probably my alltime favorite movie a story of ...          1   \n",
      "6  I sure would like to see a resurrection of a u...          1   \n",
      "7  This show was an amazing fresh innovative idea...          0   \n",
      "8  Encouraged by the positive comments about this...          0   \n",
      "9  If you like original gut wrenching laughter yo...          1   \n",
      "\n",
      "                                           sequences  \n",
      "0  [one, of, the, other, reviewers, has, mentione...  \n",
      "1  [a, wonderful, little, production, br, the, fi...  \n",
      "2  [i, thought, this, was, a, wonderful, way, to,...  \n",
      "3  [basically, theres, a, family, where, a, littl...  \n",
      "4  [petter, matteis, love, in, the, time, of, mon...  \n",
      "5  [probably, my, alltime, favorite, movie, a, st...  \n",
      "6  [i, sure, would, like, to, see, a, resurrectio...  \n",
      "7  [this, show, was, an, amazing, fresh, innovati...  \n",
      "8  [encouraged, by, the, positive, comments, abou...  \n",
      "9  [if, you, like, original, gut, wrenching, laug...  \n"
     ]
    }
   ],
   "source": [
    "file['sequences'] = file['review'].apply(lambda x: text_to_word_sequence(x))\n",
    "print(file.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...          1   \n",
      "1  A wonderful little production br The filming t...          1   \n",
      "2  I thought this was a wonderful way to spend ti...          1   \n",
      "3  Basically theres a family where a little boy J...          0   \n",
      "4  Petter Matteis Love in the Time of Money is a ...          1   \n",
      "5  Probably my alltime favorite movie a story of ...          1   \n",
      "6  I sure would like to see a resurrection of a u...          1   \n",
      "7  This show was an amazing fresh innovative idea...          0   \n",
      "8  Encouraged by the positive comments about this...          0   \n",
      "9  If you like original gut wrenching laughter yo...          1   \n",
      "\n",
      "                                           sequences  \\\n",
      "0  [one, of, the, other, reviewers, has, mentione...   \n",
      "1  [a, wonderful, little, production, br, the, fi...   \n",
      "2  [i, thought, this, was, a, wonderful, way, to,...   \n",
      "3  [basically, theres, a, family, where, a, littl...   \n",
      "4  [petter, matteis, love, in, the, time, of, mon...   \n",
      "5  [probably, my, alltime, favorite, movie, a, st...   \n",
      "6  [i, sure, would, like, to, see, a, resurrectio...   \n",
      "7  [this, show, was, an, amazing, fresh, innovati...   \n",
      "8  [encouraged, by, the, positive, comments, abou...   \n",
      "9  [if, you, like, original, gut, wrenching, laug...   \n",
      "\n",
      "                                           tokenized  \n",
      "0  [[27], [4], [1], [76], [1925], [43], [1056], [...  \n",
      "1  [[3], [383], [115], [357], [102], [1], [1350],...  \n",
      "2  [[9], [195], [10], [12], [3], [383], [97], [5]...  \n",
      "3  [[659], [212], [3], [236], [112], [3], [115], ...  \n",
      "4  [[], [34457], [110], [7], [1], [58], [4], [291...  \n",
      "5  [[235], [52], [3722], [497], [16], [3], [65], ...  \n",
      "6  [[9], [244], [56], [37], [5], [63], [3], [8815...  \n",
      "7  [[10], [118], [12], [32], [485], [1412], [4030...  \n",
      "8  [[8333], [31], [1], [1124], [761], [41], [10],...  \n",
      "9  [[42], [21], [37], [209], [7413], [9007], [219...  \n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(file.sequences)\n",
    "file['tokenized'] = file['sequences'].apply(lambda x: tokenizer.texts_to_sequences(x))\n",
    "print(file.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...          1   \n",
      "1  A wonderful little production br The filming t...          1   \n",
      "2  I thought this was a wonderful way to spend ti...          1   \n",
      "3  Basically theres a family where a little boy J...          0   \n",
      "4  Petter Matteis Love in the Time of Money is a ...          1   \n",
      "5  Probably my alltime favorite movie a story of ...          1   \n",
      "6  I sure would like to see a resurrection of a u...          1   \n",
      "7  This show was an amazing fresh innovative idea...          0   \n",
      "8  Encouraged by the positive comments about this...          0   \n",
      "9  If you like original gut wrenching laughter yo...          1   \n",
      "\n",
      "                                           sequences  \\\n",
      "0  [one, of, the, other, reviewers, has, mentione...   \n",
      "1  [a, wonderful, little, production, br, the, fi...   \n",
      "2  [i, thought, this, was, a, wonderful, way, to,...   \n",
      "3  [basically, theres, a, family, where, a, littl...   \n",
      "4  [petter, matteis, love, in, the, time, of, mon...   \n",
      "5  [probably, my, alltime, favorite, movie, a, st...   \n",
      "6  [i, sure, would, like, to, see, a, resurrectio...   \n",
      "7  [this, show, was, an, amazing, fresh, innovati...   \n",
      "8  [encouraged, by, the, positive, comments, abou...   \n",
      "9  [if, you, like, original, gut, wrenching, laug...   \n",
      "\n",
      "                                           tokenized  \\\n",
      "0  [[27], [4], [1], [76], [1925], [43], [1056], [...   \n",
      "1  [[3], [383], [115], [357], [102], [1], [1350],...   \n",
      "2  [[9], [195], [10], [12], [3], [383], [97], [5]...   \n",
      "3  [[659], [212], [3], [236], [112], [3], [115], ...   \n",
      "4  [[80322], [34457], [110], [7], [1], [58], [4],...   \n",
      "5  [[235], [52], [3722], [497], [16], [3], [65], ...   \n",
      "6  [[9], [244], [56], [37], [5], [63], [3], [8815...   \n",
      "7  [[10], [118], [12], [32], [485], [1412], [4030...   \n",
      "8  [[8333], [31], [1], [1124], [761], [41], [10],...   \n",
      "9  [[42], [21], [37], [209], [7413], [9007], [219...   \n",
      "\n",
      "                                          token_dict  \n",
      "0  {27: 1, 4: 7, 1: 16, 76: 2, 1925: 1, 43: 1, 10...  \n",
      "1  {3: 4, 383: 1, 115: 2, 357: 2, 102: 3, 1: 16, ...  \n",
      "2  {9: 3, 195: 2, 10: 5, 12: 4, 3: 6, 383: 1, 97:...  \n",
      "3  {659: 1, 212: 2, 3: 10, 236: 1, 112: 1, 115: 1...  \n",
      "4  {80322: 1, 34457: 2, 110: 1, 7: 6, 1: 20, 58: ...  \n",
      "5  {235: 1, 52: 3, 3722: 1, 497: 1, 16: 2, 3: 4, ...  \n",
      "6  {9: 4, 244: 1, 56: 5, 37: 2, 5: 5, 63: 1, 3: 6...  \n",
      "7  {10: 4, 118: 6, 12: 2, 32: 1, 485: 1, 1412: 1,...  \n",
      "8  {8333: 1, 31: 1, 1: 7, 1124: 1, 761: 1, 41: 1,...  \n",
      "9  {42: 2, 21: 4, 37: 2, 209: 1, 7413: 1, 9007: 1...  \n"
     ]
    }
   ],
   "source": [
    "file['token_dict'] = file['tokenized'].apply(lambda x: counter(x))\n",
    "print(file.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-23eb8116682b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_selector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/opencv/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3847\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3848\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3849\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-23eb8116682b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_selector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0bb50a1d9e81>\u001b[0m in \u001b[0;36mfeature_selector\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "file['features'] = file['token_dict'].apply(lambda x: feature_selector(x))\n",
    "print(file.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
